{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "### Used Packages \n",
    "* [tslearn](https://tslearn.readthedocs.io/en/stable/quickstart.html)\n",
    "* Numpy\n",
    "* Pandas\n",
    "* SKlearn\n",
    "\n",
    "### Not currently used packages, but might use\n",
    "* [DBA: Dynamic Time Warping Barycenter Averaging](https://github.com/fpetitjean/DBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [OpenFace 2.0](https://github.com/TadasBaltrusaitis/OpenFace/wiki): Facial Behavior Analysis Toolkit Tadas BaltruÅ¡aitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency, IEEE International Conference on Automatic Face and Gesture Recognition, 2018\n",
    "* Cassisi, C., Montalto, P., Aliotta, M., Cannata, A., & Pulvirenti, A. (2012). _Similarity measures and dimensionality reduction techniques for time series data mining_. Advances in data mining knowledge discovery and applications, 71-96.\n",
    "* McKinney, W. (2012). _Python for data analysis: Data wrangling with Pandas, NumPy, and IPython_. \" O'Reilly Media, Inc.\".\n",
    "* Shokoohi-Yekta, M., Hu, B., Jin, H., Wang, J., & Keogh, E. _Generalizing Dynamic Time Warping to the Multi-Dimensional Case Requires an Adaptive Approach (SDM 2015)_. In 2015 SIAM International Conference on Data Mining.-http://www. cs. ucr. edu/~ eamonn/Multi-Dimensional_DTW_Journal. pdf (last access: 18.12. 2015).\n",
    "* [TsLearn](https://tslearn.readthedocs.io/en/stable/gettingstarted.html) documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3.ipynb               \u001b[1m\u001b[36mbackup\u001b[m\u001b[m/                \u001b[1m\u001b[36mutilities\u001b[m\u001b[m/\n",
      "CreateDataset.ipynb    \u001b[1m\u001b[36mmedia\u001b[m\u001b[m/                 \u001b[1m\u001b[36mvenv\u001b[m\u001b[m/\n",
      "\u001b[1m\u001b[36mGIFGIF_Dataset\u001b[m\u001b[m/        requirements.txt\n",
      "GIFGIF_Download.ipynb  run_openface.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import glob\n",
    "from typing import *\n",
    "import umap\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tslearn.utils as tsutils\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.clustering import TimeSeriesKMeans, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regenerate all data\n",
    "set `REGENERATE` to true if you want to regenerate all data. If set to false, data saved to disk will be used. Regenerating all the data may be time-consuming, so be warned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take up to 1 hour\n",
    "REGENERATE_DATASET = False\n",
    "\n",
    "# Can take up to 2 hours\n",
    "REGENERATE_BIC = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "\n",
    "As I am going through my semester working through 4 higher-division classes and 20 hours of part-time work per week, my emotions have shifted from peace to a combination of sadness, anger and powerlessness. In order to keep my attitude positive, I chose the better of the three: sadness. \n",
    "\n",
    "I was looking to run OpenFace on the gifs, but that is apparently not possible. After spending enough hours trying to install OpenFace on my machine through 3 different methods given in the Wiki (native MacOS, `bamos/openface` docker image, `algebr/openface` docker image and creating my own docker version), I decided to go with the existing docker image from `algebr/openface`, using the method [described here](https://github.com/TadasBaltrusaitis/OpenFace/wiki). I had issues with all of the other methods.\n",
    "\n",
    "I converted all gifs to images, copied them to the docker container and and ran OpenFace on the sequences of images.\n",
    "```bash=\n",
    "docker run -it -d --rm algebr/openface:latest\n",
    "docker cp images 3a73fbce562e:/home/openface-build\n",
    "docker exec -it 3a73fbce562e sh\n",
    "docker cp run_openface.py 3a73fbce562e:/home/openface-build\n",
    "```\n",
    "I then ran my `run_openface.py` script to create the CSVs out of the image folders, and copied the results to my local machine:\n",
    "```bash\n",
    "docker cp  40b27:/home/openface-build/output ./output\n",
    "```\n",
    "\n",
    "My first look at the processed data seem to imply that OpenFace failed to parse all of the cartoons. It did show partial information, but instances of cartoons may need to be dropped from the dataset. Only images of real humans were fully labelled.\n",
    "\n",
    "_NOTE: not sure how to submite my assignment with my dataset as it is 2GB_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "I am not certain how to approach a multivariate DTW problem. It seems like there are two main approaches [(2015, Shokoohi-Yekta et al)](https://link.springer.com/article/10.1007/s10618-016-0455-0):\n",
    "1. Independent DTW: find the distance between every dimension for two time series. For multi-dimensional time series (MDT) Q and C, $$DTW_1(Q, C) = \\sum_{m=1}^{M} DTW(Q_m, C_m)$$ Each dimension is independent.\n",
    "2. Dependent DTW: warp all dimensions in a single warping matrix. The dimensions are now dependent. It's similar to single-dimension DTW, except the distance is measured for M data-points. The following distance function is used: $$d(q_i, c_j) = \\sum_{m=1}^{M} DTW(q_{i, m}, c_{j, m})^2$$\n",
    "\n",
    "### Areas of interest in the data\n",
    "[See here for in-depth explanations about output format of OpenFace](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Output-Format)\n",
    "\n",
    "[OpenFace action units information](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units)\n",
    "\n",
    "#### All columns in the CSVs\n",
    "\n",
    "\n",
    "\n",
    "| Column                         | Description                                                  | Notes                                                        | Selected |\n",
    "| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------- |\n",
    "| `gaze_0_x, gaze_0_y, gaze_0_z` | eye 0, leftmost eye in image                                 |                                                              | N        |\n",
    "| `gaze_1_x, gaze_1_y, gaze_1_z` | Eye 1, rightmost eye in image                                |                                                              | N        |\n",
    "| `gaze_angle_x, gaze_angle_y`   | Eye gaze direction in radians in world coordinates averaged for both eyes and converted into more easy to use format than gaze vectors | Might want to use that over `gaze`, as `gaze` is containted in it. | Y        |\n",
    "| `eye_lmk_x*, eye_lmk_X*`       | location of 3D eye in pixels (x) and mm (X)                  | Redundant if using head rotation and translation             | N        |\n",
    "| `pose_Rx, pose_Ry, pose_Rz`    | pitch, yaw and roll for head in radians in relation to camera | Head rotation may be useful                                  | Y        |\n",
    "| `pose_Tx, pose_Ty, pose_Tz`    | location of the head with respect to camera in millimeters   | Head translation may be useful                               | Y        |\n",
    "| `AU_r`                         | Action units intesity of 17 AUs (from 0 to 5)                | Definitely useful                                            | Y        |\n",
    "| `AU_c`                         | Action unit presence of 18 AUs                               | Definitely useful                                            | Y        |\n",
    "\n",
    "#### Fourier Transform\n",
    "Should I apply a Fourier transform on the time series, and run DTW on the output, or should I simply run DTW on the current time series?\n",
    "\n",
    "#### Number of features\n",
    "The model may overfit if too many features are provided. Try at least 3 different amount of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess time series \n",
    "* Normalize to have zero mean and unit variance\n",
    "* [Resample?](https://tslearn.readthedocs.io/en/stable/gen_modules/preprocessing/tslearn.preprocessing.TimeSeriesResampler.html#tslearn.preprocessing.TimeSeriesResampler)\n",
    "\n",
    "Apply UMAP to related dimensions to decrease number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make UMAP reproducible by using the same reducer with a set random state\n",
    "reducer = umap.UMAP(random_state=42, n_components=2)\n",
    "\n",
    "def apply_umap(pd: List[List]):\n",
    "    \"\"\"Return a table with less columns by reducing dimensions\"\"\"\n",
    "    # Apply UMAP\n",
    "    # print(pd.head())\n",
    "    reducer.fit(pd)\n",
    "    embedding = reducer.transform(pd)\n",
    "    print(f\"New embedding column number: {len(embedding[0, :])}\")\n",
    "    return pandas.DataFrame.from_records(embedding)\n",
    "\n",
    "def preprocess(df, AU_only) -> List[Any]:\n",
    "    \"\"\"\n",
    "    * Normalize to have zero mean and unit variance for every dimension.\n",
    "    * Select wanted dimensions.\n",
    "    \"\"\"\n",
    "    if AU_only:\n",
    "        df = df[[\n",
    "                    \"AU01_r\",\n",
    "                    \"AU02_r\",\n",
    "                    \"AU04_r\",\n",
    "                    \"AU05_r\",\n",
    "                    \"AU06_r\",\n",
    "                    \"AU07_r\",\n",
    "                    \"AU09_r\",\n",
    "                    \"AU10_r\",\n",
    "                    \"AU12_r\",\n",
    "                    \"AU14_r\",\n",
    "                    \"AU15_r\",\n",
    "                    \"AU17_r\",\n",
    "                    \"AU20_r\",\n",
    "                    \"AU23_r\",\n",
    "                    \"AU25_r\",\n",
    "                    \"AU26_r\",\n",
    "                    \"AU45_r\",\n",
    "                    \"AU28_c\" # This one not contained in AU*_r\n",
    "        ]].copy()\n",
    "    else:\n",
    "        df = df[[\n",
    "                    \"gaze_angle_x\",\n",
    "                    \"gaze_angle_y\",\n",
    "                    \"pose_Rx\",\n",
    "                    \"pose_Ry\",\n",
    "                    \"pose_Rz\",\n",
    "                    \"pose_Tx\",\n",
    "                    \"pose_Ty\",\n",
    "                    \"pose_Tz\",\n",
    "                    \"AU01_r\",\n",
    "                    \"AU02_r\",\n",
    "                    \"AU04_r\",\n",
    "                    \"AU05_r\",\n",
    "                    \"AU06_r\",\n",
    "                    \"AU07_r\",\n",
    "                    \"AU09_r\",\n",
    "                    \"AU10_r\",\n",
    "                    \"AU12_r\",\n",
    "                    \"AU14_r\",\n",
    "                    \"AU15_r\",\n",
    "                    \"AU17_r\",\n",
    "                    \"AU20_r\",\n",
    "                    \"AU23_r\",\n",
    "                    \"AU25_r\",\n",
    "                    \"AU26_r\",\n",
    "                    \"AU45_r\",\n",
    "                    \"AU28_c\" # This one not contained in AU*_r\n",
    "        ]].copy()\n",
    "    \n",
    "    df = pandas.DataFrame(preprocessing.scale(df), columns=df.columns)\n",
    "    \n",
    "    # Let's apply UMAP to the whole time series at once\n",
    "    df_new = apply_umap(df)\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroids_prototype(centroids, X):\n",
    "    \"\"\"Return one or more prototype for each centroid\"\"\"\n",
    "    prototypes = {i: {\"dist\": float(\"inf\"), \"index\": 0} for i in range(len(centroids))}\n",
    "\n",
    "    for i, _ in enumerate(centroids):\n",
    "        for j, _ in enumerate(X):\n",
    "            dist = dtw(centroids[i], X[j])\n",
    "            if dist < prototypes[i][\"dist\"]:\n",
    "                prototypes[i] = {\"dist\": dist, \"index\": j}\n",
    "    return prototypes\n",
    "\n",
    "def centroids_prototypes(centroids, X, num_protos=4):\n",
    "    \"\"\"Return more than one prototype for each centroid\"\"\"\n",
    "    if num_protos == 1:\n",
    "        print(\"number of centroids must be greater than 1\")\n",
    "        return\n",
    "    prototypes: List[Tuple(float, int)] = []\n",
    "\n",
    "    for i, _ in enumerate(centroids):\n",
    "        tmp = []\n",
    "        for j, _ in enumerate(X):\n",
    "            dist = dtw(centroids[i], X[j])\n",
    "            tmp.append((dist, j))\n",
    "        tmp.sort(key=lambda tup: tup[0])\n",
    "        prototypes.append(tmp[:num_protos])\n",
    "            \n",
    "    return prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time series datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_EXT = \".csv\"\n",
    "TIME_SERIES_DATASETS_PATH = \"GIFGIF_Dataset/time_series_datasets\"\n",
    "\n",
    "def extract_time_series_dataset(path: str, AU_only=False) -> List[Any]:\n",
    "    \"\"\"Create a time series dataset from all CSVs located at in \n",
    "    a directory\n",
    "    :param path: path of the directory\n",
    "    \"\"\"\n",
    "    if path[-1] == \"/\":\n",
    "        path = path[:-1]\n",
    "    dirs = glob.glob(f\"{path}/*\")\n",
    "\n",
    "    # Save every gif's openface dataset in a pandas dataframe.\n",
    "    time_series_list = []\n",
    "    \n",
    "    backup_file_name = path.split(\"/\")[-1]\n",
    "    backup_file_path = f\"{TIME_SERIES_DATASETS_PATH}/{backup_file_name}.txt\"\n",
    "\n",
    "    print(f\"Creating dataset for {path}\")\n",
    "\n",
    "    # Keep track of number of created time series\n",
    "    created_count = 0\n",
    "    # time_series_labels show the path to the OpenFace dir\n",
    "    time_series_labels = []\n",
    "\n",
    "    for dir in dirs:\n",
    "        filename = dir.split(\"/\")[-1]\n",
    "        csv_filepath = f\"{dir}/{filename}{CSV_EXT}\"\n",
    "        with open(csv_filepath) as f:\n",
    "            df = pandas.read_csv(f, dtype=float)\n",
    "\n",
    "            # Column names sometimes have an extra space on the left or right\n",
    "            df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "            if df.loc[:, \"confidence\"].mean() < 0.8:\n",
    "                continue\n",
    "\n",
    "            # Remove all NA rows\n",
    "            df.dropna()\n",
    "\n",
    "            # Select only wanted columns and scale\n",
    "            df_new = preprocess(df, AU_only)\n",
    "\n",
    "            formatted_time_series = tsutils.to_time_series(df_new)\n",
    "            time_series_list.append(formatted_time_series)\n",
    "            time_series_labels.append(dir)\n",
    "            created_count += 1\n",
    "\n",
    "    time_series_dataset = tsutils.to_time_series_dataset(time_series_list)\n",
    "\n",
    "    print(f\"Saving dataset for {path} to file {backup_file_path}\")\n",
    "    tsutils.save_time_series_txt(backup_file_path, time_series_dataset)\n",
    "    print(f\"Created {created_count} time series for time series dataset\")\n",
    "    return time_series_dataset, time_series_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "source": [
    "## Load Dataset\n",
    "Load the generated CSVs into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.84 ms, sys: 2.05 ms, total: 5.89 ms\n",
      "Wall time: 7.55 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "BACKUP = \"backup\"\n",
    "\n",
    "OPENFACE_DATA = \"GIFGIF_Dataset/openface\"\n",
    "\n",
    "X_file = f\"{BACKUP}/X\"\n",
    "X_labels_file = f\"{BACKUP}/X_labels\"\n",
    "\n",
    "X = None\n",
    "X_labels = None\n",
    "    \n",
    "if REGENERATE_DATASET:\n",
    "    X, X_labels = extract_time_series_dataset(f\"{OPENFACE_DATA}/sadness/\", AU_only=False)\n",
    "    \n",
    "    numpy.save(X_file, X)\n",
    "    numpy.save(X_labels_file, X_labels)\n",
    "else:\n",
    "    # Read from file\n",
    "    X = numpy.load(X_file + \".npy\")\n",
    "    X_labels = numpy.load(X_labels_file + \".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "X_AU_only_file = f\"{BACKUP}/X_AU_only\"\n",
    "X_AU_only_labels_file = f\"{BACKUP}/X_AU_only_labels\"\n",
    "\n",
    "X_AU_only = None\n",
    "X_AU_only_labels = None\n",
    "\n",
    "if REGENERATE_DATASET:\n",
    "    X_AU_only, X_AU_only_labels = extract_time_series_dataset(f\"{OPENFACE_DATA}/sadness/\", AU_only=True)\n",
    "    numpy.save(X_AU_only_file, X_AU_only)\n",
    "    numpy.save(X_AU_only_labels_file, X_AU_only_labels)\n",
    "else:\n",
    "    # Read from file\n",
    "    X_AU_only = numpy.load(X_AU_only_file + \".npy\")\n",
    "    X_AU_only_labels = numpy.load(X_AU_only_labels_file + \".npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Coefficients\n",
    "The higher the silhouette coefficient, the better the clustering. The silhouette coefficient [is defined as](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient):\n",
    "* a: The mean distance between a sample and all other points in the same class.\n",
    "* b: The mean distance between a sample and all other points in the next nearest cluster.\n",
    "The silhouette coefficient for a sample $s$ is given by: $$s = \\frac{b-a}{max(a, b)}$$\n",
    "The silhouette coefficient for a set is the mean of the silhouette coefficient of every member in that set.\n",
    "\n",
    "More info on silhouette coefficients from [sklearn](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%time\n",
    "\n",
    "def calculate_BIC(X, labels):\n",
    "    num_clusters = numpy.arange(2, 14)\n",
    "    sil: List[float] = []\n",
    "        \n",
    "    for num_cluster in num_clusters:\n",
    "        km = TimeSeriesKMeans(n_clusters=num_cluster, metric=\"dtw\").fit(X)\n",
    "        labels = km.labels_\n",
    "        score = silhouette_score(X, labels, metric=\"dtw\")\n",
    "        sil.append(score)\n",
    "    \n",
    "    return sil, X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIA = \"media\"\n",
    "\n",
    "def plt_BIC(sil, X, labels):\n",
    "    plt.plot(num_clusters, sil)\n",
    "    plt.title('Silhouette Scores')\n",
    "    plt.ylabel('silhouette score')\n",
    "    plt.xlabel('number of cluster')\n",
    "    plt.grid(True)\n",
    "    # Save plot\n",
    "    plt.savefig(f\"{MEDIA}/silhoutte_score.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGENERATE_BIC:\n",
    "    plt_BIC(calculate_BIC(X, X_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGENERATE_BIC:\n",
    "    plt_BIC(calculate_BIC(X_AU_only, X_AU_only_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><img src=\"media/silhoutte_score.png\" alt=\"Silhouette Scores for 2 to 10 clusters\"></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "km_2_clusters = TimeSeriesKMeans(n_clusters=2, metric=\"dtw\").fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "km_4_clusters = TimeSeriesKMeans(n_clusters=4, metric=\"dtw\").fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype for each centroid\n",
    "I am not certain my method is correct here. I kept the 2 and 4 KMeans models to look at the prototype of each centroid. What I call prototype is the point closest to each centroid, it is the best representant of that centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 clusters\n",
    "TODO: DESCRIBE THE EMOTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_2 = km_2_clusters.cluster_centers_\n",
    "prototypes_2 = centroids_prototype(centroids_2, X)\n",
    "multiple_prototypes_2 = centroids_prototypes(centroids_2, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multiple_prototypes_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenFace was set to detect single faces only. Gifs that include more than one face only contain landmarks and action units for one face. An OpenFace aligned image is provided to determine which person is being classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.image as mpimg\n",
    "from IPython.display import Image, Video\n",
    "\n",
    "\n",
    "GIFS_SADNESS = \"GIFGIF_Dataset/gifs\"\n",
    "\n",
    "def prototype_images(prototypes):\n",
    "    # Getting dict of single proto\n",
    "    if type(prototypes[0]) == dict:\n",
    "        for key, _ in enumerate(prototypes):\n",
    "                print(f\"distance: {prototypes[key]['dist']}\")\n",
    "                # Get OpenDace data path from prototype index\n",
    "                openface_dir = X_labels[(prototypes[key][\"index\"])]\n",
    "                gif_name = openface_dir.split(\"/\")[-1]\n",
    "                print(gif_name)\n",
    "\n",
    "                gif_path = f\"{GIFS_SADNESS}/{gif_name}.gif\"\n",
    "\n",
    "                with open(gif_path,'rb') as file:\n",
    "                    display(Image(file.read()))\n",
    "                print(f\"{openface_dir}/{gif_name}.avi\")\n",
    "                with open(f\"{openface_dir}/{gif_name}_aligned/frame_det_00_000001.bmp\",'rb') as file:\n",
    "                    display(Image(file.read()))\n",
    "                    \n",
    "    # Getting list of multiple protos\n",
    "    else:\n",
    "        for key, cluster in enumerate(prototypes):\n",
    "            print(\"\\n####################################################################\")\n",
    "            print(f\"CLUSTER {key}\")\n",
    "            print(\"####################################################################\")\n",
    "            for example in cluster:\n",
    "                \n",
    "                print(f\"distance: {example[0]}\")\n",
    "                # Get OpenDace data path from prototype index\n",
    "                openface_dir = X_labels[example[1]]\n",
    "                gif_name = openface_dir.split(\"/\")[-1]\n",
    "                gif_path = f\"{GIFS_SADNESS}/{gif_name}.gif\"\n",
    "\n",
    "                with open(gif_path,'rb') as file:\n",
    "                    display(Image(file.read()))\n",
    "                print(f\"{openface_dir}/{gif_name}.avi\")\n",
    "                with open(f\"{openface_dir}/{gif_name}_aligned/frame_det_00_000001.bmp\",'rb') as file:\n",
    "                    display(Image(file.read()))\n",
    "                \n",
    "                \n",
    "#prototype_images(prototypes_2)\n",
    "prototype_images(multiple_prototypes_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_4 = km_4_clusters.cluster_centers_\n",
    "prototypes_4 = centroids_prototype(centroids_4, X)\n",
    "multiple_prototypes_4 = centroids_prototypes(centroids_4, X)\n",
    "\n",
    "# prototype_images(prototypes_4)\n",
    "prototype_images(multiple_prototypes_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Leo Audibert"
   }
  ],
  "description": "",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "nteract-on-jupyter@2.1.3"
  },
  "tags": [],
  "title": "A3: Social Signals in Dynamic Data"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
